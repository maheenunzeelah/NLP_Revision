{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIlwNnIGvOBD",
        "outputId": "7171a305-2e20-4190-ac61-2830bf9e2615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 2.2376\n",
            "Epoch [20/100], Loss: 1.5320\n",
            "Epoch [30/100], Loss: 1.4709\n",
            "Epoch [40/100], Loss: 1.4730\n",
            "Epoch [50/100], Loss: 1.4506\n",
            "Epoch [60/100], Loss: 1.4780\n",
            "Epoch [70/100], Loss: 1.4506\n",
            "Epoch [80/100], Loss: 1.4683\n",
            "Epoch [90/100], Loss: 1.4326\n",
            "Epoch [100/100], Loss: 1.4242\n",
            "Predicted word: barked\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset (sentences)\n",
        "corpus = [\"the cat sat on the mat\", \"the dog barked at the cat\", \"the cat meowed at the dog\"]\n",
        "\n",
        "# Tokenization\n",
        "tokens = [sentence.split() for sentence in corpus]\n",
        "vocab = list(set(word for sentence in tokens for word in sentence))\n",
        "vocab_size = len(vocab)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "# Hyperparameters\n",
        "window_size = 2  # Number of previous words to consider\n",
        "embedding_dim = 10\n",
        "hidden_dim = 20\n",
        "batch_size = 2\n",
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Prepare dataset\n",
        "train_data = []\n",
        "for sentence in tokens:\n",
        "    for i in range(len(sentence) - window_size):\n",
        "        context = [word_to_idx[sentence[j]] for j in range(i, i + window_size)]\n",
        "        target = word_to_idx[sentence[i + window_size]]\n",
        "        train_data.append((context, target))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    contexts, targets = zip(*batch)\n",
        "    return torch.tensor(contexts, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "dataloader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Neural Network Model\n",
        "class FixedWindowNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, window_size):\n",
        "        super(FixedWindowNNLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(window_size * embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).view(x.shape[0], -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = FixedWindowNNLM(vocab_size, embedding_dim, hidden_dim, window_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for contexts, targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(contexts)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Testing the model\n",
        "with torch.no_grad():\n",
        "    test_context = torch.tensor([[word_to_idx[\"the\"], word_to_idx[\"dog\"]]], dtype=torch.long)\n",
        "    output = model(test_context)\n",
        "    predicted_idx = torch.argmax(output, dim=1).item()\n",
        "    print(\"Predicted word:\", idx_to_word[predicted_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKHwCUX6GmxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}